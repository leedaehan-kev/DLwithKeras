{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17cde0c3",
   "metadata": {},
   "source": [
    "# 오늘은 LeNet 구조를 만들어봅시다\n",
    "\n",
    "\n",
    "LeNet 구조는 CNN이며, 초기에 만들어진 모델입니다. \n",
    "\n",
    "2가지 모델(Sigmoid, ReLU)를 만들어 두 모델의 성능을 비교해봅시다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aff3fd",
   "metadata": {},
   "source": [
    "## 1.우선 필요 라이브러리를 import 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd17ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798bac6b",
   "metadata": {},
   "source": [
    "## 2. 딥러닝 모델을 설계할 때 활용하는 장비 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9880ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.11.0  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5590af",
   "metadata": {},
   "source": [
    "## 3. MNIST 데이터 다운로드 \n",
    "\n",
    " 1. Training data와 Test data 분리하기\n",
    " \n",
    " 2. Training data를 Training data 와 Validation data로 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00908077",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor(),                    # 데이터를 Tensor 형태로 변형 (PIL Image / numpy.ndarray 를 Tesnor 로 변형)\n",
    "     transforms.Normalize(mean = (0.1307,), std = (0.3081,)),     # 데이터 정규화 (Grayscale 이므로, Channel 은 1)\n",
    "    ])\n",
    "\n",
    "train_data = datasets.MNIST('./MNIST_DATASET',train=True,download=True,transform=transform)  # MNIST 데이터 셋 다운로드/변형\n",
    "test_data = datasets.MNIST('./MNIST_DATASET',train=False,download=True,transform=transform)        #채우세요\n",
    "##--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "train, val = torch.utils.data.random_split(train_data,[int(0.8*len(train_data)), int(0.2*len(train_data))])   #8:2로 분리\n",
    "                                                                         \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=BATCH_SIZE,shuffle=False,drop_last=True) \n",
    "val_loader = torch.utils.data.DataLoader(dataset=val, batch_size=BATCH_SIZE,shuffle=False,drop_last=True)     \n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH_SIZE,shuffle=False,drop_last=True)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ffb80",
   "metadata": {},
   "source": [
    "## 4. torch.nn을 이용하여 모델-1 만들기\n",
    "\n",
    "   1) 아래의 그림 중 LeNet 구조를 구현 할 것\n",
    "   \n",
    "   2) Sigmoid 활성화 함수를 이용할 것\n",
    "   \n",
    "   \n",
    "![](Comparison_image_neural_networks.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defacffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model_1(nn.Module):                  # Pytorch 에서의 신경망 정의 (torch.nn.Module 을 반드시 상속)\n",
    "    \n",
    "    def __init__(self):                  \n",
    "        super(Model_1, self).__init__()    \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, padding=2, kernel_size=5, stride=1) \n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)   \n",
    "        self.fc1 = nn.Linear(16*5*5,120)   # 은닉층 (완전연결 층)으로,120 개의 Neuron 존재, 이때, Input 으로는 직전 Pooling 의 Output을 선형화한 16x5x5 개의 데이터들\n",
    "        self.fc2 = nn.Linear(120, 84)      # 은닉층 (완전연결 층)으로,84 개의 Neuron 존재 \n",
    "        self.fc3 = nn.Linear(84, 10)       # 은닉층 (완전연결 층)으로,10 개의 Neuron 존재\n",
    "        \n",
    "    def forward(self,x):                   # 순전파 함수 Overriding \n",
    "        x = nn.functional.max_pool2d(torch.sigmoid(self.conv1(x)), (2, 2)) # Convolution Layer 1의 Output 을 Maxpooling 계층으로 연결\n",
    "        x = nn.functional.max_pool2d(torch.sigmoid(self.conv2(x)), (2, 2))  # Convolution Layer 2의 Output 을 Maxpooling 계층으로 연결\n",
    "        #print(x.shape())\n",
    "        x = x.view(-1,5*5*16)                                   # 다음, 완전연결 층에서, 행렬 곱 연산이 가능토록, Input Reshape\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "                                           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26eed9",
   "metadata": {},
   "source": [
    "## 5. torch.nn을 이용하여 모델-2 만들기\n",
    "\n",
    "   LeNet 모델에서 ReLU 활성화 함수를 사용하시요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ac70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_2, self).__init__()    \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, padding=2, kernel_size=5, stride=1)  \n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)           \n",
    "        self.fc1 = nn.Linear(16*5*5,120)   # 은닉층 (완전연결 층)으로,120 개의 Neuron 존재, 이때, Input 으로는 직전 Pooling 의 Output을 선형화한 16x5x5 개의 데이터들\n",
    "        self.fc2 = nn.Linear(120, 84)      # 은닉층 (완전연결 층)으로,84 개의 Neuron 존재 \n",
    "        self.fc3 = nn.Linear(84, 10)       # 은닉층 (완전연결 층)으로,10 개의 Neuron 존재\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = nn.functional.max_pool2d(nn.functional.relu(self.conv1(x)), (2, 2)) # Convolution Layer 1의 Output 을 Maxpooling 계층으로 연결\n",
    "        x = nn.functional.max_pool2d(nn.functional.relu(self.conv2(x)), (2, 2))  # Convolution Layer 2의 Output 을 Maxpooling 계층으로 연결\n",
    "        x = x.view(-1,5*5*16)                                   # 다음, 완전연결 층에서, 행렬 곱 연산이 가능토록, Input Reshape\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de556825",
   "metadata": {},
   "source": [
    "## 7. 학습 준비하기\n",
    "\n",
    "1) 1 epoch를 학습할 수 있는 함수 만들기\n",
    "\n",
    "2) Test와 Validation data의 정확도 계산할 수 있는 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06030b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(train_loader, network, loss_func, optimizer, epoch): # 1 Epoch 학습 할 수 있는 함수\n",
    "    '''\n",
    "    trainer_loader : Training Sample 에 대한 DataLoader\n",
    "    network : 앞서 제작한 LeNET 신경망 객체\n",
    "    loss_func : Loss 계산에 사용할 nn.functional 소속 메소드 (Loss 함수)\n",
    "    optimizer :\n",
    "    epoch : \n",
    "    '''\n",
    "    \n",
    "    train_losses = []                     # 현재 학습 단게에서의 Loss 값\n",
    "    train_correct = 0                     # 학습 정확도 0으로 초기화\n",
    "    log_interval = 300                    # 300 개의 Batch 단위로 Loss 출력\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        '''\n",
    "        batch_idx : Training Sample 에 포함된 N 개의 Batch 들 중 몇 번째 Batch 인가?\n",
    "        image : i 번째 Batch 에 대한 Input Data\n",
    "        label : Input Data 에 대한 정답 Label\n",
    "        '''\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        # 미분값의 초기화\n",
    "        optimizer.zero_grad() \n",
    "        # 미분을 통해 얻은 기울기를 0으로 초기화함. 기울기를 초기화해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있음\n",
    "\n",
    "        # Forward propagration 계산하기.\n",
    "        outputs = network.forward(image)\n",
    "        # Input Data 를 이용한 순전파 실시\n",
    "        \n",
    "        \n",
    "        # Cross_entropy 함수를 적용하여 loss를 구하고 저장하기\n",
    "        loss = loss_func(outputs,label)       # 최종 결과값과 Label 값 비교하여 Loss 계산 (Cross Entropy 계산)         \n",
    "        train_losses.append(loss.item())                # 각 Input Data들에 대한 Loss 계산하여, Append\n",
    "\n",
    "        # training accuracy 정확도 구하기 위해 맞는 샘플 개수 세기\n",
    "        pred = torch.argmax(outputs.data, 1)            \n",
    "        train_correct += pred.eq(label).sum()           # pred.eq(label) : pred, label 과 Element-Wise Level 동등 비교\n",
    "        # Gradinet 구하기\n",
    "        loss.backward()  \n",
    "\n",
    "        # weight값 update 하기\n",
    "        optimizer.step()                # 인수로 들어갔던 W와 b에서 리턴되는 변수들의 기울기에 학습률(learining rate) 0.01을 곱하여 빼줌으로서 업데이트\n",
    "\n",
    "        # 학습 상황 출력\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.6f}'\n",
    "                  .format(epoch, batch_idx * len(label), len(train_loader.dataset),100. * batch_idx / len(train_loader),\n",
    "                          loss.item()))\n",
    "            \n",
    "    return train_losses, train_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3c0dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(test_loader, network, loss_func, val = False): # 1 회 Test 할 수 있는 함수\n",
    "    '''\n",
    "    test_loader : Training Sample 에 대한 DataLoader\n",
    "    network : 앞서 제작한 LeNET 신경망 객체\n",
    "    loss_func : Loss 계산에 사용할 nn.functional 소속 메소드 (Loss 함수)\n",
    "    val : Validation 데이터에 대한 Testing (True), Testing 데이터에 대한 Testing (False)\n",
    "    '''\n",
    "    correct = 0        # Test 정확도 0으로 초기화\n",
    "    \n",
    "    test_losses = [] \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, label) in enumerate(test_loader):          # Test Sample 에 포함된 각각의 Batch 들 마다 테스트 진행\n",
    "            \n",
    "            image, label = image.to(device), label.to(device)\n",
    "\n",
    "            # Forward propagration 계산하기.\n",
    "            outputs = network.forward(image)\n",
    "\n",
    "            # Cross_entropy 함수를 적용하여 loss를 구하기\n",
    "            loss = loss_func(outputs,label) \n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "            # Batch 별로 정확도 구하기\n",
    "            pred = torch.argmax(outputs.data, 1)\n",
    "            correct += pred.eq(label).sum()\n",
    "\n",
    "        # 전체 정확도 구하기\n",
    "        test_accuracy = 100. * correct / len(test_loader.dataset)         # 백분율로 환산할 것!\n",
    "\n",
    "        \n",
    "        if val is True:                                                  # Validation 데이터에 대한 정확도 출력 (백분율로!)\n",
    "                print('Validation set: Accuracy: {}/{} ({:.2f}%)\\n'\n",
    "              .format(correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "        else:\n",
    "            print('Test set: Accuracy: {}/{} ({:.2f}%)\\n'                # Test 데이터에 대한 정확도 출력 (백분율로!)\n",
    "                  .format(correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "    return test_losses, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d73c53",
   "metadata": {},
   "source": [
    "## 8. 위 정의된 함수로 학습 함수 만들기\n",
    "\n",
    "Adam Optimizer를 사용하여 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df29783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(network, learning_rate = 0.001):\n",
    "    \n",
    "    epoches = 15                  # 전체 50000 개의 Data Sample 들에 대한 학습을 1 Epoch 라 가정,총 15 Ecpoch 학습 실시\n",
    "    \n",
    "    cls_loss = nn.CrossEntropyLoss()                                                # 손실 함수로 Cross Entropy 사용\n",
    "    optimizer = optim.Adam(network.parameters(), lr=learning_rate, weight_decay=0.1) \n",
    "    \n",
    "    '''\n",
    "    SGD 학습 사용 (전체 50000//64 Batch 에서 임의의 하나의 Batch 선택하여 학습)\n",
    "    학습률 (0.001) 사용\n",
    "    가중치 감소 비율 (0.1) 사용\n",
    "    '''\n",
    "    \n",
    "    train_losses_per_epoch = []                                 # 1 Epoch 학습 당 손실\n",
    "    test_losses_per_epoch = []                                  # 1 Epoch Test 당 손실   \n",
    "    \n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(epoches):                               # 15 Epoch 만큼 학습 실시\n",
    "                \n",
    "        # 모델를 학습 중이라고 선언하기\n",
    "        network.train()                                       \n",
    "        \n",
    "        train_losses, train_correct = training_epoch(train_loader,network,cls_loss,optimizer, epoch)\n",
    "        \n",
    "        # epoch 별로 loss 평균값, 정확도 구하기\n",
    "        average_loss = np.mean(train_losses)\n",
    "        train_losses_per_epoch.append(average_loss)\n",
    "        \n",
    "        train_accuracy = train_correct / len(train_loader.dataset) * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # epoch 별로 정확도 출력\n",
    "        print('\\nTraining set: Accuracy: {}/{} ({:.2f}%)'\n",
    "              .format(train_correct, len(train_loader.dataset),100. * train_correct / len(train_loader.dataset)))\n",
    "\n",
    "        \n",
    "        ### 학습 중에 test 결과 보기\n",
    "        \n",
    "        # 모델 test 중인 것을 선언하기\n",
    "        network.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            test_losses, test_accuracy = test_epoch(val_loader, network, cls_loss, True)\n",
    "\n",
    "        test_losses_per_epoch.append(np.mean(test_losses))\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        test_losses, test_accuracy = test_epoch(test_loader, network, cls_loss, False)\n",
    "        \n",
    "    return train_losses_per_epoch, test_losses_per_epoch, train_accuracies, test_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1394321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/48000 (0.00%)]\tLoss: 2.397290\n",
      "Train Epoch: 0 [19200/48000 (40.00%)]\tLoss: 2.323895\n",
      "Train Epoch: 0 [38400/48000 (80.00%)]\tLoss: 2.260111\n",
      "\n",
      "Training set: Accuracy: 4925/48000 (10.26%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0.00%)]\tLoss: 2.323536\n",
      "Train Epoch: 1 [19200/48000 (40.00%)]\tLoss: 2.324300\n",
      "Train Epoch: 1 [38400/48000 (80.00%)]\tLoss: 2.260082\n",
      "\n",
      "Training set: Accuracy: 4919/48000 (10.25%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 2 [0/48000 (0.00%)]\tLoss: 2.323750\n",
      "Train Epoch: 2 [19200/48000 (40.00%)]\tLoss: 2.324445\n",
      "Train Epoch: 2 [38400/48000 (80.00%)]\tLoss: 2.260014\n",
      "\n",
      "Training set: Accuracy: 4920/48000 (10.25%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 3 [0/48000 (0.00%)]\tLoss: 2.323849\n",
      "Train Epoch: 3 [19200/48000 (40.00%)]\tLoss: 2.324513\n",
      "Train Epoch: 3 [38400/48000 (80.00%)]\tLoss: 2.260004\n",
      "\n",
      "Training set: Accuracy: 4917/48000 (10.24%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 4 [0/48000 (0.00%)]\tLoss: 2.323937\n",
      "Train Epoch: 4 [19200/48000 (40.00%)]\tLoss: 2.324517\n",
      "Train Epoch: 4 [38400/48000 (80.00%)]\tLoss: 2.259986\n",
      "\n",
      "Training set: Accuracy: 4930/48000 (10.27%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 5 [0/48000 (0.00%)]\tLoss: 2.324018\n",
      "Train Epoch: 5 [19200/48000 (40.00%)]\tLoss: 2.324441\n",
      "Train Epoch: 5 [38400/48000 (80.00%)]\tLoss: 2.259969\n",
      "\n",
      "Training set: Accuracy: 4931/48000 (10.27%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 6 [0/48000 (0.00%)]\tLoss: 2.324104\n",
      "Train Epoch: 6 [19200/48000 (40.00%)]\tLoss: 2.324317\n",
      "Train Epoch: 6 [38400/48000 (80.00%)]\tLoss: 2.259988\n",
      "\n",
      "Training set: Accuracy: 4931/48000 (10.27%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 7 [0/48000 (0.00%)]\tLoss: 2.324176\n",
      "Train Epoch: 7 [19200/48000 (40.00%)]\tLoss: 2.324176\n",
      "Train Epoch: 7 [38400/48000 (80.00%)]\tLoss: 2.260024\n",
      "\n",
      "Training set: Accuracy: 4933/48000 (10.28%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 8 [0/48000 (0.00%)]\tLoss: 2.324228\n",
      "Train Epoch: 8 [19200/48000 (40.00%)]\tLoss: 2.324014\n",
      "Train Epoch: 8 [38400/48000 (80.00%)]\tLoss: 2.260014\n",
      "\n",
      "Training set: Accuracy: 4933/48000 (10.28%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 9 [0/48000 (0.00%)]\tLoss: 2.324273\n",
      "Train Epoch: 9 [19200/48000 (40.00%)]\tLoss: 2.323878\n",
      "Train Epoch: 9 [38400/48000 (80.00%)]\tLoss: 2.259990\n",
      "\n",
      "Training set: Accuracy: 4932/48000 (10.27%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 10 [0/48000 (0.00%)]\tLoss: 2.324287\n",
      "Train Epoch: 10 [19200/48000 (40.00%)]\tLoss: 2.323792\n",
      "Train Epoch: 10 [38400/48000 (80.00%)]\tLoss: 2.259970\n",
      "\n",
      "Training set: Accuracy: 4929/48000 (10.27%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 11 [0/48000 (0.00%)]\tLoss: 2.324283\n",
      "Train Epoch: 11 [19200/48000 (40.00%)]\tLoss: 2.323747\n",
      "Train Epoch: 11 [38400/48000 (80.00%)]\tLoss: 2.259961\n",
      "\n",
      "Training set: Accuracy: 4927/48000 (10.26%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 12 [0/48000 (0.00%)]\tLoss: 2.324278\n",
      "Train Epoch: 12 [19200/48000 (40.00%)]\tLoss: 2.323727\n",
      "Train Epoch: 12 [38400/48000 (80.00%)]\tLoss: 2.259958\n",
      "\n",
      "Training set: Accuracy: 4929/48000 (10.27%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 13 [0/48000 (0.00%)]\tLoss: 2.324275\n",
      "Train Epoch: 13 [19200/48000 (40.00%)]\tLoss: 2.323719\n",
      "Train Epoch: 13 [38400/48000 (80.00%)]\tLoss: 2.259956\n",
      "\n",
      "Training set: Accuracy: 4929/48000 (10.27%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Train Epoch: 14 [0/48000 (0.00%)]\tLoss: 2.324274\n",
      "Train Epoch: 14 [19200/48000 (40.00%)]\tLoss: 2.323716\n",
      "Train Epoch: 14 [38400/48000 (80.00%)]\tLoss: 2.259955\n",
      "\n",
      "Training set: Accuracy: 4929/48000 (10.27%)\n",
      "Validation set: Accuracy: 1185/12000 (9.88%)\n",
      "\n",
      "Test set: Accuracy: 980/10000 (9.80%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = Model_1().to(device)\n",
    "rlt_const = training(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64815daf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/48000 (0.00%)]\tLoss: 2.324073\n",
      "Train Epoch: 0 [19200/48000 (40.00%)]\tLoss: 0.844715\n",
      "Train Epoch: 0 [38400/48000 (80.00%)]\tLoss: 0.400623\n",
      "\n",
      "Training set: Accuracy: 34568/48000 (72.02%)\n",
      "Validation set: Accuracy: 10402/12000 (86.68%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0.00%)]\tLoss: 0.435991\n",
      "Train Epoch: 1 [19200/48000 (40.00%)]\tLoss: 0.471510\n",
      "Train Epoch: 1 [38400/48000 (80.00%)]\tLoss: 0.345997\n",
      "\n",
      "Training set: Accuracy: 43135/48000 (89.86%)\n",
      "Validation set: Accuracy: 10679/12000 (88.99%)\n",
      "\n",
      "Train Epoch: 2 [0/48000 (0.00%)]\tLoss: 0.362746\n",
      "Train Epoch: 2 [19200/48000 (40.00%)]\tLoss: 0.375910\n",
      "Train Epoch: 2 [38400/48000 (80.00%)]\tLoss: 0.328153\n",
      "\n",
      "Training set: Accuracy: 43863/48000 (91.38%)\n",
      "Validation set: Accuracy: 10905/12000 (90.88%)\n",
      "\n",
      "Train Epoch: 3 [0/48000 (0.00%)]\tLoss: 0.324384\n",
      "Train Epoch: 3 [19200/48000 (40.00%)]\tLoss: 0.367007\n",
      "Train Epoch: 3 [38400/48000 (80.00%)]\tLoss: 0.319711\n",
      "\n",
      "Training set: Accuracy: 44065/48000 (91.80%)\n",
      "Validation set: Accuracy: 10987/12000 (91.56%)\n",
      "\n",
      "Train Epoch: 4 [0/48000 (0.00%)]\tLoss: 0.299549\n",
      "Train Epoch: 4 [19200/48000 (40.00%)]\tLoss: 0.370286\n",
      "Train Epoch: 4 [38400/48000 (80.00%)]\tLoss: 0.305318\n",
      "\n",
      "Training set: Accuracy: 44163/48000 (92.01%)\n",
      "Validation set: Accuracy: 10998/12000 (91.65%)\n",
      "\n",
      "Train Epoch: 5 [0/48000 (0.00%)]\tLoss: 0.289809\n",
      "Train Epoch: 5 [19200/48000 (40.00%)]\tLoss: 0.366379\n",
      "Train Epoch: 5 [38400/48000 (80.00%)]\tLoss: 0.297531\n",
      "\n",
      "Training set: Accuracy: 44244/48000 (92.18%)\n",
      "Validation set: Accuracy: 10983/12000 (91.53%)\n",
      "\n",
      "Train Epoch: 6 [0/48000 (0.00%)]\tLoss: 0.292962\n",
      "Train Epoch: 6 [19200/48000 (40.00%)]\tLoss: 0.358586\n",
      "Train Epoch: 6 [38400/48000 (80.00%)]\tLoss: 0.293225\n",
      "\n",
      "Training set: Accuracy: 44281/48000 (92.25%)\n",
      "Validation set: Accuracy: 10976/12000 (91.47%)\n",
      "\n",
      "Train Epoch: 7 [0/48000 (0.00%)]\tLoss: 0.297618\n",
      "Train Epoch: 7 [19200/48000 (40.00%)]\tLoss: 0.349718\n",
      "Train Epoch: 7 [38400/48000 (80.00%)]\tLoss: 0.289767\n",
      "\n",
      "Training set: Accuracy: 44335/48000 (92.36%)\n",
      "Validation set: Accuracy: 10993/12000 (91.61%)\n",
      "\n",
      "Train Epoch: 8 [0/48000 (0.00%)]\tLoss: 0.298573\n",
      "Train Epoch: 8 [19200/48000 (40.00%)]\tLoss: 0.364898\n",
      "Train Epoch: 8 [38400/48000 (80.00%)]\tLoss: 0.284705\n",
      "\n",
      "Training set: Accuracy: 44353/48000 (92.40%)\n",
      "Validation set: Accuracy: 11024/12000 (91.87%)\n",
      "\n",
      "Train Epoch: 9 [0/48000 (0.00%)]\tLoss: 0.294508\n",
      "Train Epoch: 9 [19200/48000 (40.00%)]\tLoss: 0.368912\n",
      "Train Epoch: 9 [38400/48000 (80.00%)]\tLoss: 0.282369\n",
      "\n",
      "Training set: Accuracy: 44390/48000 (92.48%)\n",
      "Validation set: Accuracy: 11035/12000 (91.96%)\n",
      "\n",
      "Train Epoch: 10 [0/48000 (0.00%)]\tLoss: 0.293996\n",
      "Train Epoch: 10 [19200/48000 (40.00%)]\tLoss: 0.375049\n",
      "Train Epoch: 10 [38400/48000 (80.00%)]\tLoss: 0.278287\n",
      "\n",
      "Training set: Accuracy: 44433/48000 (92.57%)\n",
      "Validation set: Accuracy: 11044/12000 (92.03%)\n",
      "\n",
      "Train Epoch: 11 [0/48000 (0.00%)]\tLoss: 0.292674\n",
      "Train Epoch: 11 [19200/48000 (40.00%)]\tLoss: 0.361476\n",
      "Train Epoch: 11 [38400/48000 (80.00%)]\tLoss: 0.279291\n",
      "\n",
      "Training set: Accuracy: 44474/48000 (92.65%)\n",
      "Validation set: Accuracy: 11040/12000 (92.00%)\n",
      "\n",
      "Train Epoch: 12 [0/48000 (0.00%)]\tLoss: 0.291521\n",
      "Train Epoch: 12 [19200/48000 (40.00%)]\tLoss: 0.362304\n",
      "Train Epoch: 12 [38400/48000 (80.00%)]\tLoss: 0.273032\n",
      "\n",
      "Training set: Accuracy: 44510/48000 (92.73%)\n",
      "Validation set: Accuracy: 11033/12000 (91.94%)\n",
      "\n",
      "Train Epoch: 13 [0/48000 (0.00%)]\tLoss: 0.293320\n",
      "Train Epoch: 13 [19200/48000 (40.00%)]\tLoss: 0.357529\n",
      "Train Epoch: 13 [38400/48000 (80.00%)]\tLoss: 0.272308\n",
      "\n",
      "Training set: Accuracy: 44516/48000 (92.74%)\n",
      "Validation set: Accuracy: 11044/12000 (92.03%)\n",
      "\n",
      "Train Epoch: 14 [0/48000 (0.00%)]\tLoss: 0.293113\n",
      "Train Epoch: 14 [19200/48000 (40.00%)]\tLoss: 0.369953\n",
      "Train Epoch: 14 [38400/48000 (80.00%)]\tLoss: 0.270582\n",
      "\n",
      "Training set: Accuracy: 44533/48000 (92.78%)\n",
      "Validation set: Accuracy: 11051/12000 (92.09%)\n",
      "\n",
      "Test set: Accuracy: 9277/10000 (92.77%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = Model_2().to(device)\n",
    "rlt_const = training(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b1471",
   "metadata": {},
   "source": [
    "## 9. 두모델의 성능을 비교하시오"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8156f",
   "metadata": {},
   "source": [
    "정답)\n",
    "활성화 함수로 ReLu를 사용했을 때가 Sigmoid를 사용했을 때보다 월등한 성능을 가진다. \n",
    "ReLu를 사용했을 때는 epoch가 지남에 따라 학습이 제대로 되는 반면, Sigmoid는 학습이 제대로 되지 않았다.\n",
    "이는 활성화 함수의 특성 때문이다. ReLU의 수렴 속도는 Tanh 같은 함수보다 훨씬 빠르다. 또한 활성화 값을 구하기 위해서는 임계값(threshold  value)만 필요하다. 즉 다른 활성화 함수에 비해 복잡한 계산이 없고 gradient descent시에 시간을 절약할 수 있다. ReLU 기능은 NN 희소성을 초래하는 일부 뉴런의 출력을 0으로 만들고 매개 변수의 상호의존성을 감소시키며 과적합 문제를 완화하고 전체적으로 더 효율적으로 만들 수 있다.\n",
    "반면에 Sigmoid는 출력 값이 극한에 가까워 질수록 기존 S자 모양이던 곡선이 점점 수평이 되어 간다. 이것은 출력 값들을 오직 0,1에 가까운 값으로만 압축을 하는 'saturation' 현상을 초래한다. 그렇기에 입력 값이 너무 클 경우 data loss를 일으켜 학습이 진행되지 않는다.\n",
    "\n",
    "참고자료 : https://iopscience.iop.org/article/10.1088/1755-1315/428/1/012097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4bec28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
